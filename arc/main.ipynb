{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aWuXEALFvuN1",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1761437417149,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "aWuXEALFvuN1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DMw48azZWsEf",
   "metadata": {
    "executionInfo": {
     "elapsed": 3689,
     "status": "ok",
     "timestamp": 1761437467746,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "DMw48azZWsEf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f552df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2749,
     "status": "ok",
     "timestamp": 1760805714615,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "d6f552df",
    "outputId": "2263e77a-2482-4d9d-b6be-986459b510d6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pyensembl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "t3bmybjgW-Tm",
   "metadata": {
    "id": "t3bmybjgW-Tm"
   },
   "outputs": [],
   "source": [
    "import src.modelnew as m\n",
    "import src.data_cleaning as dc\n",
    "import src.position_encoding as pos\n",
    "import src.pathway_encoding as path\n",
    "import src.training as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68484c87-818b-460c-87a8-6460dabe0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /hpc/home/zy231/.cache/pyensembl/GRCh38/ensembl109/Homo_sapiens.GRCh38.cdna.all.fa.gz.pickle\n",
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /hpc/home/zy231/.cache/pyensembl/GRCh38/ensembl109/Homo_sapiens.GRCh38.ncrna.fa.gz.pickle\n",
      "INFO:pyensembl.sequence_data:Loaded sequence dictionary from /hpc/home/zy231/.cache/pyensembl/GRCh38/ensembl109/Homo_sapiens.GRCh38.pep.all.fa.gz.pickle\n"
     ]
    }
   ],
   "source": [
    "ensembl_data = pyensembl.EnsemblRelease(109)\n",
    "ensembl_data.download()\n",
    "ensembl_data.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642b95f2",
   "metadata": {
    "id": "642b95f2"
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad('vcc_data/adata_Training.h5ad')\n",
    "#adata.obs['perturbation_idx'] = np.random.randint(0, CONFIG[\"n_perturbations\"], size=adata.n_obs)\n",
    "gene_names = pd.read_csv('vcc_data/gene_names.csv', header = None)\n",
    "gene_names = gene_names.iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"n_genes\": len(gene_names), #total number of expression genes needed to predict\n",
    "    \"n_perturbations\": len(adata.obs['target_gene'].unique().tolist()), #number of unique perturbations\n",
    "    \"n_chromosomes\": 25, #chromosome number 23+X+Y\n",
    "\n",
    "    \"perturbation_dim\": 256,      # Condition embedding\n",
    "    \"chrom_embedding_dim\": 16,     # Learned in-model for chromosome identity\n",
    "    \"locus_fourier_features\": 8,   # Number of Fourier frequency pairs (2*F)\n",
    "    \"pathway_dim\": 50,             # From pre-trained Autoencoder(based on hallmark MSigDB)\n",
    "    \"gene_identity_dim\": 189,       # Main learnable gene embedding\n",
    "\n",
    "    # Backbone dims\n",
    "    \"d_model\": 512,                # Mamba hidden size\n",
    "    \"mamba_layers\": 4,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 4,\n",
    "\n",
    "    # Head\n",
    "    \"prediction_head\": \"linear\", # \"linear\" | \"probabilistic\"\n",
    "\n",
    "    # Training\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 5e-5, # Lowered LR for AdamW stability\n",
    "    \"epochs\": 100,\n",
    "}\n",
    "\n",
    "# Derived dimensions for clarity\n",
    "# Positional encoding dim = chromosome embedding + MLP output from Fourier features\n",
    "POS_DIM = CONFIG[\"chrom_embedding_dim\"] + CONFIG[\"chrom_embedding_dim\"]\n",
    "GENE_FEAT_DIM = CONFIG[\"gene_identity_dim\"] + CONFIG[\"pathway_dim\"] + POS_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de52e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2853,
     "status": "ok",
     "timestamp": 1760805739500,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "75de52e6",
    "outputId": "262db9b2-5f45-41da-93b1-51dfc11e54e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing positional indices using pyensembl ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching gene positions: 100%|██████████| 18080/18080 [03:32<00:00, 84.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUCCESS: Generated positional tensors with shapes:\n",
      "Chromosome Indices (chr_idx): torch.Size([18080])\n",
      "Normalized Locus (locus_norm): torch.Size([18080, 1])\n",
      "Locus Fourier Features (locus_fourier): torch.Size([18080, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_perturbations = adata.obs['target_gene'].unique().tolist()\n",
    "perturbation_to_idx_map = {name: i for i, name in enumerate(unique_perturbations)}\n",
    "adata.obs['perturbation_idx'] = adata.obs['target_gene'].map(perturbation_to_idx_map)\n",
    "control_adata = dc.get_control_data(adata)\n",
    "chr_idx, locus_norm, locus_fourier = pos.precompute_positional_indices(ensembl_data, gene_names, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75adb9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560220,
     "status": "ok",
     "timestamp": 1760806299722,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "c75adb9a",
    "outputId": "99b945f2-6a77-455a-a05d-9926e4a963b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Precomputing pathway features on control data ---\n",
      "AE epoch 10/50 | recon MSE: 25.4281\n",
      "AE epoch 20/50 | recon MSE: 10.0758\n",
      "AE epoch 30/50 | recon MSE: 7.7874\n",
      "AE epoch 40/50 | recon MSE: 7.2923\n",
      "AE epoch 50/50 | recon MSE: 7.1357\n",
      "Generated pathway_features shape: torch.Size([18080, 50])\n"
     ]
    }
   ],
   "source": [
    "    # Pre-compute Fixed Features (run once, then save/load)\n",
    "#filtered_data = dc.clean_and_preprocess_data(adata)\n",
    "    # find dataset that can represent a nnoramla scell set\n",
    "pathway_feats = path.precompute_pathway_features(control_adata, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abb5c1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18008,
     "status": "ok",
     "timestamp": 1760806317752,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "7abb5c1f",
    "outputId": "3dfc5a2e-0580-4ada-851a-ba37138487f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "Dataset using log-normalized data from adata.X for linear head.\n"
     ]
    }
   ],
   "source": [
    "    # Instantiate Model, Dataset, Dataloader\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "model = m.TranscriptomePredictor(CONFIG, GENE_FEAT_DIM, pathway_feats, chr_idx, locus_fourier)\n",
    "model.to(device)\n",
    "\n",
    "dataset = m.PerturbationDataset(adata, CONFIG) # Pass CONFIG to the dataset constructor\n",
    "train_loader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = len(dataset) - train_size\n",
    "\n",
    "# 2. Perform the split\n",
    "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 3. Create separate DataLoaders for each set\n",
    "#train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "#print(f\"Data split into {len(train_dataset)} training samples and {len(val_dataset)} validation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85a3e56-6151-46a4-bb34-fac0f29b04bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking every dataset entry for NaNs (Slow Method) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318af476d79a48e7b2d7ce4c25516307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning dataset:   0%|          | 0/221273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OK: Scanned all {total_samples} samples. The dataset is 100% clean of NaNs.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm # For a progress bar\n",
    "\n",
    "print(\"\\n--- Checking every dataset entry for NaNs (Slow Method) ---\")\n",
    "\n",
    "nan_found_at_index = -1\n",
    "total_samples = len(dataset)\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for i in tqdm(range(total_samples), desc=\"Scanning dataset\"):\n",
    "    sample = dataset[i]\n",
    "    target_expr = sample[\"target_expression\"]\n",
    "    \n",
    "    # Check if this tensor has any NaN values\n",
    "    if torch.isnan(target_expr).any():\n",
    "        nan_found_at_index = i\n",
    "        break # Stop as soon as we find one\n",
    "\n",
    "# --- Report the result ---\n",
    "if nan_found_at_index != -1:\n",
    "    print(\"\\n!!!!! CRITICAL: NaN DETECTED !!!!!\")\n",
    "    print(f\"A NaN value was found in the 'target_expression' for sample index: {nan_found_at_index}\")\n",
    "    print(\"This is the likely cause of your CUDA error.\")\n",
    "else:\n",
    "    print(\"\\nOK: Scanned all {total_samples} samples. The dataset is 100% clean of NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6m9fS5lU6bhS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 1753,
     "status": "error",
     "timestamp": 1760806319510,
     "user": {
      "displayName": "Steve Yin",
      "userId": "14953835441563414104"
     },
     "user_tz": 240
    },
    "id": "6m9fS5lU6bhS",
    "outputId": "e40eef13-fda9-41c3-98e5-70c3c76f0940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clear_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mprint(f\"\\nUsing device: {device}\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mmodel = TranscriptomePredictor(CONFIG, GENE_FEAT_DIM, pathway_feats, chr_idx, locus_fourier).to(device)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mdataset = m.PerturbationDataset(adata, CONFIG) # Pass CONFIG to the dataset constructor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mdataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)'''\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mtrain_model(model, train_loader, CONFIG,device)\n",
      "File \u001b[0;32m/work/zy231/virtual_cell/src/training.py:90\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, config, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m     avg_epoch_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m     88\u001b[0m     epoch_loss_history\u001b[38;5;241m.\u001b[39mappend(avg_epoch_loss)\n\u001b[0;32m---> 90\u001b[0m     plot_training_loss(epoch_loss_history,avg_epoch_loss)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving trained model weights...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/zy231/virtual_cell/src/training.py:34\u001b[0m, in \u001b[0;36mplot_training_loss\u001b[0;34m(epoch_loss_history, avg_epoch_loss)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03mPlots the training loss over epochs.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    epoch_losses (list): A list of the average loss for each epoch.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Clear the output of the current cell to prepare for the new plot\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     37\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(epoch_loss_history) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), epoch_loss_history, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clear_output' is not defined"
     ]
    }
   ],
   "source": [
    "'''device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "model = TranscriptomePredictor(CONFIG, GENE_FEAT_DIM, pathway_feats, chr_idx, locus_fourier).to(device)\n",
    "\n",
    "dataset = m.PerturbationDataset(adata, CONFIG) # Pass CONFIG to the dataset constructor\n",
    "dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)'''\n",
    "\n",
    "    # Train\n",
    "loss_history = train.train_model(model, train_loader, CONFIG,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2b444-2a3d-4952-b587-cef19e5dbabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ad912-a389-4897-a463-28094feee925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d36dd-d674-484a-8218-e8614ba2d20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cell_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
